# Machine Learning

## Here are the laboratory exercises I have done in this course. They are in croatian but the english version is on the way. There are two tasks one in each folder. Exercises are written in Jupyter Notebook format.

## Exercise 1: Linear Regression 

### Topics covered in this exercise are:
- Simple Linear Regression
- Polynomial regression and the effect of noise
- Model selection and overfitting/underfitting
- Regularization: Ridge (L2) and Lasso (L1)
- Feature scaling and multicollinearity
- Analysis of model stability

#### We implemented regression models using analytical solutions and scikit-learn. Then we compared different complexity models and regularization techniques. We analyzed the stability of the regression coefficients under regularization and feature scaling.

---

## Exercise 2: Linear Discriminative models and Logistic Regression

### Topics covered in this exercise are:

- Linear regression as a classifier
- RidgeClassifier and LinearRegression for classification
- Multiclass classification (one-vs-rest scheme)
- Logistic regression: theory and implementation
- Gradient descent optimization for logistic regression
- Regularization in logistic regression
- Feature mapping and non-linear decision boundaries

#### We trained linear classifiers using both regression and logistic regression approaches. We implemented logistic regression with batch gradient descent and analyzed the effects of regularization and feature mapping on classification performance. Then we compared the results with scikit-learn's LogisticRegression and explored the effects of outliers on model performance.

---

## Requirements

- Python 3.x
- Jupyter Notebook
- libraries: numpy, matplotlib, scikit-learn

---

## Usage

Open the Jupyter Notebooks in the respective folders and run the cells sequentially to see the implementations and results.